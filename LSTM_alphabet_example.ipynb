{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a -> b\n",
      "b -> c\n",
      "c -> d\n",
      "d -> e\n",
      "e -> f\n",
      "f -> g\n",
      "g -> h\n",
      "h -> i\n",
      "i -> j\n",
      "j -> k\n",
      "k -> l\n",
      "l -> m\n",
      "m -> n\n",
      "n -> o\n",
      "o -> p\n",
      "p -> q\n",
      "q -> r\n",
      "r -> s\n",
      "s -> t\n",
      "t -> u\n",
      "u -> v\n",
      "v -> w\n",
      "w -> x\n",
      "x -> y\n",
      "y -> z\n",
      "[[[ 0.        ]]\n",
      "\n",
      " [[ 0.03846154]]\n",
      "\n",
      " [[ 0.07692308]]\n",
      "\n",
      " [[ 0.11538462]]\n",
      "\n",
      " [[ 0.15384615]]\n",
      "\n",
      " [[ 0.19230769]]\n",
      "\n",
      " [[ 0.23076923]]\n",
      "\n",
      " [[ 0.26923077]]\n",
      "\n",
      " [[ 0.30769231]]\n",
      "\n",
      " [[ 0.34615385]]\n",
      "\n",
      " [[ 0.38461538]]\n",
      "\n",
      " [[ 0.42307692]]\n",
      "\n",
      " [[ 0.46153846]]\n",
      "\n",
      " [[ 0.5       ]]\n",
      "\n",
      " [[ 0.53846154]]\n",
      "\n",
      " [[ 0.57692308]]\n",
      "\n",
      " [[ 0.61538462]]\n",
      "\n",
      " [[ 0.65384615]]\n",
      "\n",
      " [[ 0.69230769]]\n",
      "\n",
      " [[ 0.73076923]]\n",
      "\n",
      " [[ 0.76923077]]\n",
      "\n",
      " [[ 0.80769231]]\n",
      "\n",
      " [[ 0.84615385]]\n",
      "\n",
      " [[ 0.88461538]]\n",
      "\n",
      " [[ 0.92307692]]]\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "# create mapping from char to int\n",
    "char_to_int = dict((c,i) for i,c in enumerate(alphabet))\n",
    "int_to_char = dict((i,c) for i,c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input ot output pairs encoded as integers\n",
    "seq_length = 1\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0,len(alphabet)-seq_length, 1):\n",
    "    seq_in = alphabet[i:i+seq_length]\n",
    "    seq_out = alphabet[i+seq_length]\n",
    "    X.append([char_to_int[char] for char in seq_in])\n",
    "    Y.append([char_to_int[seq_out]])\n",
    "    print seq_in, '->', seq_out\n",
    "\n",
    "# LSTM format: samples, time steps, features\n",
    "X = np.reshape(X,(len(X),seq_length,1))\n",
    "# normalize \n",
    "X = X/float(len(alphabet))\n",
    "print X\n",
    "# value to binary data\n",
    "Y = np_utils.to_categorical(Y)\n",
    "print Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy: 88.00%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# first param 32 units,\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "model.fit(X,Y, epochs = 500, batch_size = 1, verbose = 0)\n",
    "\n",
    "scores = model.evaluate(X,Y,verbose=0)\n",
    "print ('model accuracy: %.2f%%' % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc -> d\n",
      "bcd -> e\n",
      "cde -> f\n",
      "def -> g\n",
      "efg -> h\n",
      "fgh -> i\n",
      "ghi -> j\n",
      "hij -> k\n",
      "ijk -> l\n",
      "jkl -> m\n",
      "klm -> n\n",
      "lmn -> o\n",
      "mno -> p\n",
      "nop -> q\n",
      "opq -> r\n",
      "pqr -> s\n",
      "qrs -> t\n",
      "rst -> u\n",
      "stu -> v\n",
      "tuv -> w\n",
      "uvw -> x\n",
      "vwx -> y\n",
      "wxy -> z\n",
      "model accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input ot output pairs encoded as integers\n",
    "seq_length = 3\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0,len(alphabet)-seq_length, 1):\n",
    "    seq_in = alphabet[i:i+seq_length]\n",
    "    seq_out = alphabet[i+seq_length]\n",
    "    X.append([char_to_int[char] for char in seq_in])\n",
    "    Y.append([char_to_int[seq_out]])\n",
    "    print seq_in, '->', seq_out\n",
    "\n",
    "# LSTM format: samples, time steps, features\n",
    "# The difference is that the reshaping of the input data takes the sequence\n",
    "# as a time step sequence of one feature, rather than a single time step of \n",
    "# multiple features\n",
    "\n",
    "X = np.reshape(X,(len(X),seq_length,1))\n",
    "# normalize \n",
    "X = X/float(len(alphabet))\n",
    "# value to binary data\n",
    "Y = np_utils.to_categorical(Y)\n",
    "\n",
    "model = Sequential()\n",
    "# first param 32 units,\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "model.fit(X,Y, epochs = 500, batch_size = 1, verbose = 0)\n",
    "\n",
    "scores = model.evaluate(X,Y,verbose=0)\n",
    "print ('model accuracy: %.2f%%' % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
